{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pyspark.sql.functions as sf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First preparation (streams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# All POMS data\n",
    "poms = spark.read.parquet(\"gs://mit-processed-events-prod.npo-data.nl/poms-enriched/\")\n",
    "print(\"Total poms: \", poms.count())\n",
    "\n",
    "# Filter on broadcast and availability of locations\n",
    "poms = poms.filter((sf.col(\"type\")==\"BROADCAST\") & (sf.size(\"locations\") > 0))\n",
    "print(\"Filtered poms: \", poms.count())\n",
    "\n",
    "# Select the possible used features\n",
    "poms = poms.select([c for c in poms.columns if c in ['age_rating', 'broadcaster', 'credits', 'descriptions', 'genres', 'mid', 'locations', 'sortDate', 'titles']])\n",
    "poms.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def locations(df):\n",
    "    stream = []\n",
    "    for index, row in df.iterrows():\n",
    "        stream_bool = None\n",
    "        for location in row['locations']:\n",
    "            if (location.program_url.startswith(\"npo+drm://\") or location.program_url.startswith(\"npo://\")) & (location.platform == \"INTERNETVOD\" or location.platform == \"PLUSVOD\") & (location.publish_start != '0' or location.publish_stop != '0'):\n",
    "                stream_bool = True\n",
    "                break\n",
    "        stream.append(stream_bool)\n",
    "    df['stream'] = stream\n",
    "\n",
    "locations(df)\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poms2 = spark.createDataFrame(df)\n",
    "poms2 = poms2.drop('stream')\n",
    "poms2 = poms2.drop('locations')\n",
    "poms2.write.parquet(\"gs://dataproc-jupyter-eileen.npo-data.nl/data/poms_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poms = spark.read.parquet(\"gs://dataproc-jupyter-eileen.npo-data.nl/data/poms_data/\")\n",
    "print(poms.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = poms.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptions(df):\n",
    "    descriptions = []\n",
    "    for index, row in df.iterrows():\n",
    "        descript = None\n",
    "        for description in row['descriptions']:\n",
    "            if (description.type == \"MAIN\"):\n",
    "                descript = description.value\n",
    "            elif (description.type == \"SHORT\"):\n",
    "                descript = description.value\n",
    "            elif (description.type == \"KICKER\"):\n",
    "                descript = description.value\n",
    "            else:\n",
    "                continue\n",
    "        descriptions.append(descript)\n",
    "    df['description'] = descriptions\n",
    "\n",
    "def titles(df):\n",
    "    titles = []\n",
    "    for index, row in df.iterrows():\n",
    "        title = None\n",
    "        for title in row['titles']:\n",
    "            if (title.type == \"MAIN\"):\n",
    "                title = title.value\n",
    "                break;\n",
    "            else:\n",
    "                continue\n",
    "        titles.append(title)\n",
    "    df['title'] = titles\n",
    "\n",
    "descriptions(df)\n",
    "titles(df)\n",
    "df = df.drop(columns='descriptions')\n",
    "df = df.drop(columns='titles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poms2 = spark.createDataFrame(df)\n",
    "\n",
    "def replace_age(column, value):\n",
    "    return sf.when(column != value, column).otherwise(sf.lit(\"ALL\"))\n",
    "\n",
    "poms2 = poms2.withColumn(\"age_rating\", replace_age(sf.col(\"age_rating\"), \"\"))\n",
    "poms2.write.parquet(\"gs://dataproc-jupyter-eileen.npo-data.nl/data/poms_data_v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation subtitles (with partitioning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "poms = spark.read.parquet(\"gs://dataproc-jupyter-eileen.npo-data.nl/data/poms_data_v2/\")\n",
    "poms.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poms.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poms = poms.repartition(25)\n",
    "poms.write.parquet(\"gs://dataproc-jupyter-eileen.npo-data.nl/data/poms_partition25/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run subtitle on range partitions\n",
    "def sub_extract(df):\n",
    "    subtitles = []\n",
    "    poms_endpoint = \"https://rs.poms.omroep.nl/v1/api/subtitles/\"\n",
    "    for index, row in df.iterrows():\n",
    "        mid = row['mid']\n",
    "        r = requests.get(poms_endpoint + mid + \"/nl_NL/CAPTION.vtt\")\n",
    "        sub = r.text.encode('ascii','ignore') # encode subtitles\n",
    "        sub = sub.lower().split('\\n\\n') # lower and split\n",
    "        sub = sub[1:] # remove first entry of subtitles 'webvtt'\n",
    "        sub = [line.split('\\n', 2)[-1].replace('\\n', ' ') for line in sub] # remove display time info and '\\n' in subtitle text\n",
    "        sub = u\" \".join(sub)\n",
    "        subtitles.append(sub)\n",
    "    df['sub'] = subtitles\n",
    "\n",
    "for i in range(0,25):\n",
    "    print(i)\n",
    "    poms_part = spark.read.parquet(\"gs://dataproc-jupyter-eileen.npo-data.nl/data/poms_partition25/part-000\" \n",
    "                                   + str(i) +\"-cf0443fe-b3d1-4798-866a-b044e1e85628-c000.snappy.parquet\")\n",
    "    df = poms_part.toPandas()\n",
    "    sub_extract(df)\n",
    "    poms_part2 = spark.createDataFrame(df)\n",
    "    poms_part2.write.parquet(\"gs://dataproc-jupyter-eileen.npo-data.nl/data/sub_partition25/\" + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []\n",
    "for i in range (0, 25):\n",
    "    paths.append('gs://dataproc-jupyter-eileen.npo-data.nl/data/sub_partition25/' + str(i))\n",
    "\n",
    "poms = spark.read.parquet(*paths)\n",
    "poms.write.parquet(\"gs://dataproc-jupyter-eileen.npo-data.nl/data/poms_sub/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}