{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ2\n",
    "- Main research question: _'Can a hybrid recommendation system using metadata perform better than the current recommendation system of NPO Start which uses collaborative filtering?'_\n",
    "- RQ2: _'Can the performance of the current recommendation system be improved by implementing a hybrid recommendation system?'_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context \n",
    "NPO Start is a service that offers users the ability to watch video content on demand. This video content is displayed to users in so-called \"ribbons\" or rows that have a certain theme, like 'Populair', 'Nieuw' and 'Aanbevolen voor jou'. Each ribbon consists out of a ranked list of several items. \n",
    "\n",
    "Users of the service who have an account have the ability to receive several personalized ribbons which contain items that are recommended for a specific user. These recommendations are materialized on the front page of the service in the ribbon called 'Aanbevolen voor jou'. This ribbon utilizes the history of user interactions with items to perform collaborative filtering. These user interactions are grouped on series level and evaluated by pairs of series which are frequently watched together, or coincide often, with the history of the user. Of these coincidences, the top 100 pairs are extracted which are ordered based on their frequency, which results in a personalized ribbon of items for a single user. \n",
    "\n",
    "However, there is a lot of metadata available about the offered content which is unused by the current recommendation system. In this thesis, the metadata of broadcasts will be utilised in a hybrid recommendation system to determine if it can improve the performance of the current video recommendation system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operationalization\n",
    "#### Evaluation\n",
    "To assess the quality of recommendations three metrics are evaluated: precision@k (precision at k), AP (average precision) and CTR (click-through rate).  \n",
    "\n",
    "* __Precision@k__\n",
    "Precision@k is a metric that evaluates the proportion of top-k recommended items that are relevant to the user. A relevant item is an item that was chosen by a user when it was offered in a ribbon. Relevant items are denoted as a true positive (TP) which are positive predicted values. Precision is then given as the total number of predicted positives out of all predicted items.\n",
    "\\begin{equation}\n",
    "P@k = \\frac{|\\{i \\in TP\\mid i\\mbox{ ranked in top k}\\}|}{k}\n",
    "\\end{equation}\n",
    "The precision@k over all users is given by the mean precision, where for each recommendation the corresponding precision@k is calculated and the mean is taken out of all the scores. \n",
    "\n",
    "* __AP__\n",
    "AP evaluates the quality and rank of the recommended items. This metric is lower when positive predicted values do not appear at the top of the item list. It assesses the precision at each rank and divides it by the total amount of TPs.\n",
    "\\begin{equation}\n",
    "AP = \\frac{1}{TP}\\sum_{k=1}^m\\frac{|\\{i \\in TP\\mid i\\mbox{ ranked in top k}\\}|}{k}\n",
    "\\end{equation}\n",
    "The AP over all the users is given by the MAP (mean average precision), where for each recommendation the corresponding AP is calculated and the mean is taken out of all the scores. This metric then shows how good the model is at performing the recommendations. R is the number of recommendations in the set and AP(r) is the average precision for a given recommendation, r. \n",
    "Q is the number of queries in the set and AveP(q) is the average precision (AP) for a given query, q.\n",
    "\\begin{equation}\n",
    "MAP = \\frac{\\sum_{r=1}^R AP(r)}{R}\n",
    "\\end{equation}\n",
    "\n",
    "* __CTR__ <br>\n",
    "The CTR measures the proportion of choices to click on an item of the recommended ribbon divided by the times the ribbon was offered. The equation of the click-through rate is shown in the equation below.\n",
    "\\begin{equation}\n",
    "CTR = \\frac{\\textit{number of click-throughs}}{\\textit{number of offers}}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "The higher the value of the metrics, the better. The version with the highest mean precision and CTR has the most success of recommending items that users are interested in, and the version with the highest MAP is most successful in ranking the recommendations in a personalized manner. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data\n",
    "The input data for the hybrid recommendation system consists of user interaction data and content features. \n",
    "\n",
    "The user interaction data consists of NPO event data, which has two types: offers and choices. Offers describe the ranked items that were shown in the 'Aanbevolen voor jou' ribbon to a user. Choices describe the chosen items out of offered ribbons and their rank. \n",
    "\n",
    "The content features consists of metadata of each item which is provided by the Publieke Omroep Media Service (POMS) of the NPO. \n",
    "Nine content features were chosen for the hybrid recommendation system, namely age rating, broadcaster, credits, description, genres, mid, series reference, subtitles and title. These content features are evaluated to determine which feature or combination of several features improves the recommendation system the most. These features are used for the content-based side of the recommendation by transforming them into vectors. An example of a few of the content features for the serie 'De beste zangers van Nederland' is shown below. \n",
    "\n",
    "| mid        | value                                    | feature     |\n",
    "|------------|------------------------------------------|-------------|\n",
    "| AT_2033328 | AVTR                                     | broadcaster |\n",
    "| AT_2033328 | TROS                                     | broadcaster |\n",
    "| AT_2033328 | (Reinier, Victor, PRESENTER)             | credits     |\n",
    "| AT_2033328 | (Smit, Jan, PRESENTER)                   | credits     |\n",
    "| AT_2033328 | (Rombley, Edsilia, PRESENTER)            | credits     |\n",
    "| AT_2033328 | (Veerman, Johan, DIRECTOR)               | credits     |\n",
    "| AT_2033328 | Row(id=u'3.0.1.5', terms=[u'Muziek'])    | genres      |\n",
    "| AT_2033328 | Row(id=u'3.0.1.6', terms=[u'Amusement']) | genres      |\n",
    "| AT_2033328 | zangers                                  | title       |\n",
    "| AT_2033328 | nederland                                | title       |\n",
    "| AT_2033328 | beste                                    | title       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experimental setup\n",
    "A hybrid recommendation system using the Python library LightFM was set up which is a hybrid matrix factorisation model which represents users and items as linear combinations of their content featuresâ€™ latent factors. A weighted approximate-rank pairwise loss was used which learns to rank the items. The model learns user and item representations from the input data, by using the latent representation approach. It computes recommendations for new items and users, by representing items and users as linear combinations of their content features. \n",
    "\n",
    "The steps taken in implementing the LightFM model are:\n",
    "1. Loading and cleaning the user interaction data\n",
    "    1. removing duplicates and thresholding (making sure users have liked at least 5 items, and items are liked by at least 5 users)\n",
    "2. Preparing content feature vectors\n",
    "    2. one-hot-encoding the content features\n",
    "3. Transforming the user interaction data into an interactions matrix\n",
    "4. Preparing a 5-fold cross-validation \n",
    "5. Executing the model\n",
    "6. Optimizing the hyperparameters \n",
    "    6. using the library scikit-optimize on the parameters epoch, learning rate, number of components and alpha\n",
    "\n",
    "<img style=\"width:250px;height:350px;border:0;\" src=\"model_flow.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of this model is assessed in an offline setting on historical user interaction data from the month March. Different versions of the model that incoorporate different content features are tested to assess the precision of recommendations. For now the precision@k of seven different versions has been tested, which consists out of a baseline model that only employs MF, and six others that employ either one content feature that has or hasn't been pre-processed. The different versions and the accompanied length of the encoded content vector for each user is shown below.\n",
    "\n",
    "| Version | Content features                | Encoded vector length |\n",
    "|---------|---------------------------------|-----------------------|\n",
    "| 1       | none                            | x                     |\n",
    "| 2       | broadcaster                     | 28                    |\n",
    "| 3       | genres                          | 53                    |\n",
    "| 4       | credits                         | 5689                  |\n",
    "| 5a      | title (words)                   | 4893                  |\n",
    "| 5b      | title (words without stopwords) | 4642                  |\n",
    "| 5c      | title (tf-idf)                  | 3252                  |\n",
    "| 6       | description (tf-idf)            | 13557                 |\n",
    "| 7       | sub (tf-idf)                    | 14210                 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each version described above has been executed on user interaction data of the period March 1-7. The mean precision@k (k=5) and AP has been evaluated for each one, and the results are shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Version | Content features                     | Precision@k | P@k-std | AP   | AP-std |\n",
    "|---------|--------------------------------------|-------------|---------|------|--------|\n",
    "| 1       | MF                                   | 0,09        | 0,03    | 0,26 | 0,11   |\n",
    "| 2       | MF + broadcaster                     |             |         |      |        |\n",
    "| 3       | MF + genres                          |             |         |      |        |\n",
    "| 4       | MF + credits                         |             |         |      |        |\n",
    "| 5a      | MF + title (words)                   |             |         |      |        |\n",
    "| 5b      | MF + title (words without stopwords) |             |         |      |        |\n",
    "| 5c      | MF + title (tf-idf)                  |             |         |      |        |\n",
    "| 6       | MF + description (tf-idf)            |             |         |      |        |\n",
    "| 7       | MF + sub (tf-idf)                    |             |         |      |        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results indicate that the highest precision was achieved by the model that used no content features, so which only used matrix factorization. <font color='red'>The lowest precision was gained when the content features broadcaster and title were incoorporated into the hybrid recommendation model. Both the hybrid models that only had one content feature had a precision around the 0,15 and all the versions have about the same standard deviation. </font> This shows that incooporating content features into the model did not help the precision of recommendations.\n",
    "\n",
    "One possible explanation for the lower achieved precision for the models with content features is the drawback of offline experiments. Offline experiments assume that members would have behaved the same, e.g. playing the same videos, if the new model being evaluated was used to generate the recommendations (Gomez, 2015). Thus the new models that produce different recommendations from the current NPO recommendation system are unlikely to find that their recommendations are chosen more than the actual offered recommendations. Since the user interaction data is biased towards the current recommendation system, it may affect the produced precision in an offline environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further steps\n",
    "- Method\n",
    "    - Perform model on combinations of content features \n",
    "- Results\n",
    "    - Investigate the produced lists.\n",
    "- Evaluation\n",
    "    - Investigate the distribution of P@k's and standard deviation per user (confidence interval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}