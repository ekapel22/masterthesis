{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pyspark.sql.functions import udf\n",
    "import pyspark.sql.functions as sf\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poms = spark.read.parquet(\"gs://mit-processed-events-prod.npo-data.nl/poms-enriched/\")\n",
    "print(\"Total poms: \", poms.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Gain broadcasts that are able to be streamed on NPO Start\n",
    "poms = (\n",
    "    poms\n",
    "    .filter(sf.col(\"type\")==\"BROADCAST\")\n",
    "    .filter(sf.size(\"locations\") > 0)\n",
    "    .select('broadcaster', 'credits', 'descriptions', 'genres', 'mid', 'locations', 'seriesRef', 'titles')\n",
    "    .withColumn('locations', sf.explode('locations'))\n",
    "    .withColumn('program_url', sf.col('locations.program_url'))\n",
    "    .withColumn('platform', sf.col('locations.platform'))\n",
    "    .withColumn('publish_start', sf.col('locations.publish_start'))\n",
    "    .withColumn('publish_stop', sf.col('locations.publish_stop'))\n",
    "    .filter(sf.col('program_url').startswith('npo+drm://') | sf.col('program_url').startswith('npo://'))\n",
    "    .filter((sf.col('platform') == 'INTERNETVOD') | (sf.col('platform') == 'PLUSVOD'))\n",
    "    .filter((sf.col('publish_start') != '0') | (sf.col('publish_stop') != '0'))\n",
    "    .filter(~sf.col('seriesRef').isNull())\n",
    ")\n",
    "print(\"Filtered poms: \", poms.count())\n",
    "poms.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the content features and ids\n",
    "poms = poms.select('broadcaster', 'credits', 'descriptions', 'genres', 'mid', 'seriesRef', 'titles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation of textual features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gain the right title and description (as displayed on NPO Start)\n",
    "w = Window.partitionBy('mid').orderBy(sf.length(\"description.type\"))\n",
    "\n",
    "poms = (poms\n",
    "        .withColumn('titles', sf.explode('titles'))\n",
    "        .withColumn('title_type', sf.col('titles.type'))\n",
    "        .withColumn('title', sf.col('titles.value'))\n",
    "        .filter(sf.col('title_type')=='MAIN')\n",
    "        .withColumn('description', sf.explode(\"descriptions\"))\n",
    "        .filter(sf.col('description.type').isin([\"MAIN\", \"SHORT\", \"KICKER\"]))\n",
    "        .withColumn('description', sf.first('description.value').over(w))\n",
    ")\n",
    "\n",
    "poms = poms.select('broadcaster', 'credits', 'description', 'genres', 'mid', 'seriesRef', 'title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtitle extraction using the poms api\n",
    "mid = poms.select('mid').distinct()\n",
    "\n",
    "def spark_udf(data_type):\n",
    "    def create_udf(f):\n",
    "        return udf(f, data_type)\n",
    "    return create_udf\n",
    "@spark_udf(StringType())\n",
    "def spark_sub(x):\n",
    "    sub = requests.get(\"https://rs.poms.omroep.nl/v1/api/subtitles/\" + x + \"/nl_NL/CAPTION.vtt\").text.encode('ascii','ignore')\n",
    "    sub = sub.lower().split('\\n\\n') # lower and split\n",
    "    sub = sub[1:] # remove first entry of subtitles 'webvtt'\n",
    "    sub = [line.split('\\n', 2)[-1].replace('\\n', ' ') for line in sub] # remove display time info and '\\n' in subtitle text\n",
    "    sub = u\" \".join(sub)\n",
    "    return sub\n",
    "\n",
    "poms = poms.withColumn('sub', spark_sub('mid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poms.write.parquet(\"gs://dataproc-jupyter-eileen.npo-data.nl/data/poms_stream/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature encoding\n",
    "### Categorical features\n",
    "#### Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark.sql.functions as sf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "plt.style.use('bmh')\n",
    "pd.options.display.max_columns = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poms = spark.read.parquet(\"gs://dataproc-jupyter-eileen.npo-data.nl/data/poms_stream/\")\n",
    "print(poms.count())\n",
    "print(poms.printSchema())\n",
    "df = poms.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Broadcaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'broadcaster'\n",
    "sideinfo = (\n",
    "    poms\n",
    "    .select(sf.col('seriesRef').alias('mid'), sf.col(feature).alias('value'))\n",
    "    .withColumn('feature', sf.lit(feature))\n",
    "    .dropDuplicates()\n",
    ")\n",
    "sideinfo.show()\n",
    "sideinfo.write.parquet(\"gs://dataproc-jupyter-eileen.npo-data.nl/data/content_features/\" + feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Credits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'credits'\n",
    "\n",
    "sideinfo = (\n",
    "    poms\n",
    "    .select(sf.col('seriesRef').alias('mid'), sf.col(feature).alias('value'))\n",
    "    .withColumn('value', sf.explode('value'))\n",
    "    .withColumn('feature', sf.lit(feature))\n",
    "    .withColumn('value', sf.concat(sf.col('value.family_name'),sf.lit(', '), sf.col('value.given_name')))\n",
    "    .dropDuplicates()\n",
    ")\n",
    "sideinfo.show()\n",
    "sideinfo.write.parquet(\"gs://dataproc-jupyter-eileen.npo-data.nl/data/content_features/\" + feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'genres'\n",
    "\n",
    "sideinfo = (\n",
    "    poms\n",
    "    .select(sf.col('seriesRef').alias('mid'), sf.col(feature).alias('value'))\n",
    "    .withColumn('value', sf.explode('value'))\n",
    "    .withColumn('feature', sf.lit(feature))\n",
    "    .withColumn('value', sf.col('value.terms').cast(StringType()))\n",
    "    .dropDuplicates()\n",
    ")\n",
    "sideinfo.printSchema()\n",
    "sideinfo.show()\n",
    "sideinfo.write.parquet(\"gs://dataproc-jupyter-eileen.npo-data.nl/data/content_features/\" + feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Textual features\n",
    "#### Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as sf\n",
    "pd.options.display.max_columns = 500\n",
    "\n",
    "import re as re\n",
    "from pyspark.ml.feature import CountVectorizer, IDF, StopWordsRemover, NGram, RegexTokenizer\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.nl import Dutch\n",
    "from spacy.lang.en import English\n",
    "stopwords = spacy.lang.nl.stop_words.STOP_WORDS.union(spacy.lang.en.stop_words.STOP_WORDS)\n",
    "\n",
    "from pyspark.mllib.linalg import Vector, Vectors\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poms = spark.read.parquet(\"gs://dataproc-jupyter-eileen.npo-data.nl/data/poms_stream/\")\n",
    "print(poms.count())\n",
    "poms.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform TF-IDF per feature\n",
    "| Feature     | TF-IDF amount |\n",
    "|-------------|---------------|\n",
    "| title       | 3             |\n",
    "| description | 10            |\n",
    "| subtitles   | 20            |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3 # 10, 20\n",
    "text = 'title' #'description', 'sub'\n",
    "df = poms.select(sf.col('seriesRef').alias('mid'), sf.col(text)) \n",
    "df = df.dropna(subset=text)\n",
    "df = df.groupBy(\"mid\").agg(sf.collect_set(text)).withColumn(\"text\", sf.concat_ws(\" \", \"collect_set(\"+text+\")\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML pipeline, consisting of four stages: tokenizer, stopwordremover, countvectorizer, idf\n",
    "tokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", minTokenLength = 4, pattern=\"\\\\W\")\n",
    "stopwordsList = [s.encode('utf-8') for s in stopwords]\n",
    "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"filtered\" ,stopWords=stopwordsList)\n",
    "cv = CountVectorizer(inputCol=remover.getOutputCol(), outputCol=\"rawFeatures\")\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, cv])\n",
    "model = pipeline.fit(df)\n",
    "\n",
    "df = model.transform(df)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(df)\n",
    "df = idfModel.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top n tf-idf\n",
    "def spark_udf(data_type):\n",
    "    def create_udf(f):\n",
    "        return udf(f, data_type)\n",
    "    return create_udf\n",
    "@spark_udf(ArrayType(IntegerType()))\n",
    "def spark_argmaxes(vector):\n",
    "    if len(vector.values) > 0:\n",
    "        if len(vector.values) < n:\n",
    "            return np.argpartition(vector.values, -len(vector.values)).tolist()\n",
    "        if len(vector.values) >= n:\n",
    "            return np.argpartition(vector.values, -n)[-n:].tolist()\n",
    "    return None\n",
    "df = df.withColumn('argmaxes', spark_argmaxes('features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract top-n words\n",
    "word_df = df.withColumn('argmaxes', sf.explode('argmaxes')).select('mid', 'features', 'argmaxes')\n",
    "\n",
    "vocabulary = [x.encode('UTF8') for x in model.stages[2].vocabulary]\n",
    "\n",
    "@spark_udf(StringType())\n",
    "def spark_vocab(vector, x):\n",
    "    if len(vector.values) > 0:\n",
    "        return vocabulary[vector.indices[x]]\n",
    "    return None\n",
    "word_df = word_df.withColumn('value', spark_vocab('features', 'argmaxes'))\n",
    "text_tfidf = word_df.select('mid', 'value').withColumn('feature', sf.lit(text))\n",
    "text_tfidf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tfidf.write.parquet(\"gs://dataproc-jupyter-eileen.npo-data.nl/data/content_features/\" + text + \"_tfidf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}